---
title: "HW2"
author: "Jiang Bian"
date: "`r format(Sys.time(), '%d %B %Y')`"
documentclass: article
papersize: letter
fontsize: 11pt
output: pdf_document
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
rm(list=ls())
if (!require("knitr")) 
  install.packages("knitr",dependencies = TRUE) 

## specify global chunk options
knitr::opts_chunk$set(fig.width = 16, fig.height = 5, dpi = 300,
                      out.width = "90%", fig.align = "center")
```



####Q1

##a
According to the question, we know that 
$$
p(x;\theta)=\frac{1}{\pi[1+(x-\theta)^2]}
$$

Because $x_1,...,x_n$ is an i.i.d. sample and $l(\theta)$ the log-likelihood function of $\theta$ based on the sample.
Therefore,
$$\begin{aligned}
l(\theta)&=\ln(\prod_{i=1}^n p(x_i;\theta))\\
&=\ln(\prod_{i=1}^n \frac{1}{\pi[1+(x_i-\theta)^2]})\\
&=\sum_{i=1}^n\ln(\frac{1}{\pi[1+(x_i-\theta)^2]})\\
&=-n\ln\pi-\sum_{i=1}^n\ln[1+(\theta-x_i)^2]\\
\end{aligned}$$

And,
$$
l\,'(\theta)=-2\sum_{i=1}^n\frac{\theta-x_i}{1+(\theta-x_i)^2}
$$
$$
l\,''(\theta)=-2\sum_{i=1}^n\frac{1-(\theta-x_i)^2}{[1+(\theta-x_i)^2]^2}
$$

As for $I(\theta)$,
$$
I(\theta)=n\int\frac{\left \{p'(x)\right \}^2}{p(x)}dx
$$
$$
p\,'(x)=\frac{-2(x-\theta)}{\pi[1+(x-\theta)^2]^2}
$$

Thus,
$$
I(\theta)=\frac{4n}{\pi}\int_{-\infty}^\infty\frac{x^2dx}{(1+x^2)^3}
$$

Suppose $x=\tan(t)$ where $t\in(-\frac{\pi}{2},\frac{\pi}{2})$,

$$\begin{aligned}
I(\theta)&=\frac{4n}{\pi} \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \frac{\tan^2(t)d(\tan(t))}{(1+tan^2(t))^3}\\
&=\frac{4n}{\pi}\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\frac{\tan^2(t)}
{(\frac{1}{\sec^2(t)})^3}(\frac{1}{\sec^2(t)})dt\\
&=\frac{4n}{\pi}\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\frac{\sin^2(t)}{\cos^2(t)}\cos^4(t)dt\\
&=\frac{4n}{\pi}\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\sin^2(t)\cos^2(t)dt\\
&=\frac{4n}{\pi}*\frac{\pi}{8}=\frac{n}{2}
\end{aligned}$$

So, $I(\theta)=\frac{n}{2}$.

##b
The graph of log-likelihood function.
```{r q1bgraph}
###q1b
x <- c(1.77,-0.23,2.76,3.80,3.47,56.75,-1.34,4.24,-2.44,
       3.29,3.71,-2.40,4.53,-0.07,-1.05,-13.87,-2.53,-1.75)
#l(theta)
llikeli <- function(theta,x. = x){
  sum(-log(pi)-log(1+(x. - theta)^2))
}
llike <- Vectorize(llikeli)
#graph the log-likelihood function, limit from -100 to 100
curve(llike, from = -100, to = 100, n = 10000)
```

```{r q1b, echo=FALSE}
#l'(theta)
lp <- function(theta,x. = x){
  -2*sum((theta-x.)/(1+(theta-x.)^2))
}
#l''(theta)
lpp <- function(theta,x. = x)
{
  -2*sum((1-(theta-x.)^2)/((1+(theta-x.)^2)^2))
}

#starting points
startp <- c(-11, -1, 0, 1.5, 4, 4.7, 7, 8, 38)

thetas_1b <- rep(NA,length(startp))
counts_1b <- rep(NA,length(startp))
lvals_1b <- rep(NA,length(startp))

#Newton-Raphson method
for(i in 1:length(startp)){
  theta <- startp[i]
  count <- 0
  process <- TRUE
  while(process){
    l <- llikeli(theta, x. = x)
    l1 <- lp(theta, x. = x)
    l2 <- lpp(theta, x. = x)
    #theta_(t+1) = theta_t - l'(theta_t)/l''(theta_t)
    theta <- theta - l1/l2
    count <- count + 1
    #let l1 close to 0 and test less than 10000 times
    if(abs(l1) < 1e-6 | count == 10000)
      process = FALSE
  }
  thetas_1b[i] <- theta
  counts_1b[i] <- count
  lvals_1b[i] <- llikeli(theta, x. = x)
}
```

The results of using Newton's method is shown below.
```{r addpoints}
counts_1b
lvals_1b
#determine the range of the plot
llikeli(-5, x)
max(lvals_1b)

#plotting
plot(thetas_1b,lvals_1b, "p", xlim = c(-5, 5), 
     ylim = c(-90, -60), xlab = c("thetas"), ylab = c("lvals"))
curve(llike, from = -5, to = 5, n = 1000, add = TRUE)
```

When take sample mean as the start point.

```{r samplemean, echo=FALSE}
#take sample mean as the start point
startpp <- mean(x)

thetas_1bb <- rep(NA,length(startpp))
counts_1bb <- rep(NA,length(startpp))
lvals_1bb <- rep(NA,length(startpp))

#Newton-Raphson method
for(i in 1:length(startpp)){
  theta <- startpp[i]
  count <- 0
  process <- TRUE
  while(process){
    l <- llikeli(theta, x. = x)
    l1 <- lp(theta, x. = x)
    l2 <- lpp(theta, x. = x)
    #theta_(t+1) = theta_t - l'(theta_t)/l''(theta_t)
    theta <- theta - l1/l2
    count <- count + 1
    #let l1 close to 0 and test less than 10000 times
    if(abs(l1) < 1e-6 | count == 10000)
      process = FALSE
  }
  thetas_1bb[i] <- theta
  counts_1bb[i] <- count
  lvals_1bb[i] <- llikeli(theta, x. = x)
}
```

```{r meanresult}
counts_1bb
lvals_1bb
```
When using sample mean as a start point, the count_1bb is 5 and the l value is closer to maximum value, so it is a good start point.

##c
```{r q1c, echo=FALSE}
G <- function(theta, x. = x, alpha. = alpha){
  alpha * lp(theta, x. = x) + theta
}
alphas <- c(1, 0.64, 0.25)
thetas_1c <- matrix(NA,length(startp),length(alphas))
counts_1c <- matrix(NA,length(startp),length(alphas))
lvals_1c <- matrix(NA,length(startp),length(alphas))

#fixed-point iterations
for(j in 1:length(startp)){
  theta <- startp[j]
  for (k in 1:length(alphas)){
    alpha <- alphas[k]
    count <- 0
    process <- TRUE
    while(process){
      l1 <- lp(theta, x. = x)
      #G(x) = alpha * l'(theta) + theta
      theta <- G(theta, x. = x, alpha. = alpha)
      count <- count + 1
      #let l1 close to 0 and test less than 10000 times
      if(abs(l1) < 1e-6 | count == 10000){
        process = FALSE
        thetas_1c[j,k] <- theta
        counts_1c[j,k] <- count
        lvals_1c[j,k] <- llikeli(theta, x. = x)       
      }
    }
  }
}

row.names(thetas_1c) <- startp
row.names(counts_1c) <- startp
row.names(lvals_1c) <- startp
colnames(thetas_1c) <- alphas
colnames(counts_1c) <- alphas
colnames(lvals_1c) <- alphas
```

The added points of fixed-point method is shown in the graph.

```{r q1cgraph}
plot(thetas_1b,lvals_1b, "p", xlim = c(-5, 5), 
     ylim = c(-90, -60), xlab = c("thetas"), ylab = c("lvals"))
curve(llike, from = -5, to = 5, n = 1000, add = TRUE)
typ <- seq(1,length(alphas),1)
for (m in 1:length(typ)){
  points(thetas_1c[,m], lvals_1c[,m], pch=typ[m], col=typ[m], cex=2, lwd=2)
}
```

##d
```{r q1d,echo=FALSE}
#I(theta) = n/2
I <- length(x)/2

thetas_1d <- rep(NA,length(startp))
counts_1d <- rep(NA,length(startp))
lvals_1d <- rep(NA,length(startp))

#fisher scoring
for(n in 1:length(startp)){
  theta <- startp[n]
  count <- 0
  process <- TRUE
  while(process){
    l1 <- lp(theta, x. = x)
    #theta_(t+1) = theta_t - l'(theta_t)/l''(theta_t)
    theta <- theta + l1/I
    count <- count + 1
    #let l1 close to 0 and test less than 10 times
    if(abs(l1) < 1e-6 | count == 10000)
      process = FALSE
  }
  thetas_1d[n] <- theta
  counts_1d[n] <- count
  lvals_1d[n] <- llikeli(theta, x. = x)
}

#then using newton raaphson method
thetas_1dn <- rep(NA,length(startp))
counts_1dn <- rep(NA,length(startp))
lvals_1dn <- rep(NA,length(startp))

for(i in 1:length(startp)){
  theta <- thetas_1d[i]
  count <- 0
  process <- TRUE
  while(process){
    l <- llikeli(theta, x. = x)
    l1 <- lp(theta, x. = x)
    l2 <- lpp(theta, x. = x)
    #theta_(t+1) = theta_t - l'(theta_t)/l''(theta_t)
    theta <- theta - l1/l2
    count <- count + 1
    #let l1 close to 0 and test less than 10000 times
    if(abs(l1) < 1e-6 | count == 10000)
      process = FALSE
  }
  thetas_1dn[i] <- theta
  counts_1dn[i] <- count
  lvals_1dn[i] <- llikeli(theta, x. = x)
}
```

The added points of Fisher Scoring is shown in the graph.
```{r q1dgraph}
plot(thetas_1b,lvals_1b, "p", xlim = c(-5, 5), 
     ylim = c(-90, -60), xlab = c("thetas"), ylab = c("lvals"))
curve(llike, from = -5, to = 5, n = 1000, add = TRUE)
typ <- seq(1,length(alphas),1)
for (m in 1:length(typ)){
  points(thetas_1c[,m], lvals_1c[,m], pch=typ[m], col=typ[m], cex=2, lwd=2)
}
points(thetas_1dn,lvals_1dn, pch=length(typ)+1, col=length(typ)+1, cex=2, lwd=2)
```

##e
Comparing the counts number of three different methods.
```{r counts}
counts_1b
knitr::kable(counts_1c)
counts_1d
```

And comparing the thetas of three different methods.
```{r thetas}
thetas_1b
knitr::kable(thetas_1c)
thetas_1d
```

By comparing the results we achieved from above methods, we found that Newton-Raphson method has more than half of the results faster than other methods except the results when alpha 0.25 in fixed-point iterations. But when using other start points, the counts will be more than 20 and the theta is divergent to infinite, so the stability of Newton-Raphson is not as good as other methods. 

In fixed-point iterations, it depends strongly on the value of alpha, when alpha = 1, the counts reach to the maximum counts limit. 

In Fisher Scoring, it can make rapid improvement but using more times than Newton's method.



####Q2

##a
Known the probability density with parameter is
$$
p(x;\theta)=\frac{1-\cos(x-\theta)}{2\pi}
$$
The log-likelihood function of $\theta$ is
$$
-n\ln2\pi+\sum_{i=1}^n\ln[1-\cos(x_i-\theta)]
$$

Therefore, the graph of the function between $-\pi$ and $\pi$ is,
```{r q2a}
rm(list=ls())
x <- c(3.91, 4.85, 2.28, 4.06, 3.70, 4.04, 5.46, 3.53, 2.28, 1.96,
       2.53, 3.88, 2.22, 3.47, 4.82, 2.46, 2.99, 2.54, 0.52)

#l(theta)
llikeli <- function(theta,x. = x){
  sum(-log(2*pi)+log(1-cos(x. - theta)))
}
llike <- Vectorize(llikeli)
#graph the log-likelihood function, limit from -pi to pi
curve(llike, from = -pi, to = pi, n = 1000)
```

##b
Known 
$$\begin{aligned}
E[X|\theta]&=\int_0^{2\pi}x*\frac{1-\cos(x-\theta)}{2\pi}dx\\
&=\int_0^{2\pi}\frac{x}{2\pi}dx-\int_0^{2\pi}\frac{x*\cos(x-\theta)}{2\pi}dx\\
&=\frac{1}{2\pi}*\frac{x^2}{2}|_0^{2\pi}-\int_0^{2\pi}\frac{x}{2\pi}d[\sin(x-\theta)]\\
&=\pi-\frac{x}{2\pi}*\sin(x-\theta)|_0^{2\pi}-\int_0^{2\pi}\sin(x-\theta)d(\frac{x}{2\pi})\\
&=\pi-\sin(2\pi-\theta)-\frac{1}{2\pi}\int_{-\theta}^{2\pi-\theta}\sin{u}du\\
&=\pi+\sin \theta+\frac{1}{2\pi}\cos{u}|_{-\theta}^{2\pi-\theta}\\
&=\pi+\sin \theta
\end{aligned}$$

the method-of-moments estimator of $\theta$ is, 
```{r q2b}
###q2b
estimator <-function(theta, xbar){
  pi+sin(theta)-xbar
}
print(root1<-uniroot(estimator,c(0,pi/2),xbar=mean(x))$root)

print(root2<-uniroot(estimator,c(pi/2,pi),xbar=mean(x))$root)
```

##c & d
Set the required thetas as starting points from former question and requirements.
```{r startpoints}
#starting points
startp <- c(root1, root2, -2.7, 2.7)
```

```{r q2cd,echo=FALSE}
lp <- function(theta, x. = x){
  sum((sin(x-theta))/(1-cos(x-theta)))
}
lpp <- function(theta, x. = x){
  -sum(1/(cos(x-theta)-1))
  }

thetas_2cd <- rep(NA,length(startp))
counts_2cd <- rep(NA,length(startp))
lvals_2cd <- rep(NA,length(startp))

#Newton-Raphson method
for(i in 1:length(startp)){
  theta <- startp[i]
  count <- 0
  process <- TRUE
  while(process){
    l <- llikeli(theta, x. = x)
    l1 <- lp(theta, x. = x)
    l2 <- lpp(theta, x. = x)
    #theta_(t+1) = theta_t - l'(theta_t)/l''(theta_t)
    theta <- theta - l1/l2
    count <- count + 1
    #let l1 close to 0 and test less than 10000 times
    if(abs(l1) < 1e-6 | count == 10000)
      process = FALSE
  }
  thetas_2cd[i] <- theta
  counts_2cd[i] <- count
  lvals_2cd[i] <- llikeli(theta, x. = x)
}

thetas_2cd <- matrix(thetas_2cd, ncol = length(startp))
counts_2cd <- matrix(counts_2cd, ncol = length(startp))
lvals_2cd <- matrix(lvals_2cd, ncol = length(startp))
colnames(thetas_2cd) <- round(startp, digits = 2)
colnames(counts_2cd) <- round(startp, digits = 2)
colnames(lvals_2cd) <- round(startp, digits = 2)
```

We received,
```{r q2cdr}
knitr::kable(thetas_2cd)
knitr::kable(counts_2cd)
knitr::kable(lvals_2cd)
```

##e
```{r q2e,echo=FALSE}
###q2e
#starting points
startp <- seq(-pi, pi, length.out = 200)

thetas_2e <- rep(NA,length(startp))
counts_2e <- rep(NA,length(startp))
lvals_2e <- rep(NA,length(startp))

#Newton-Raphson method
for(i in 1:length(startp)){
  theta <- startp[i]
  count <- 0
  process <- TRUE
  while(process){
    l <- llikeli(theta, x. = x)
    l1 <- lp(theta, x. = x)
    l2 <- lpp(theta, x. = x)
    #theta_(t+1) = theta_t - l'(theta_t)/l''(theta_t)
    theta <- theta - l1/l2
    count <- count + 1
    #let l1 close to 0 and test less than 10000 times
    if(abs(l1) < 1e-6 | count == 10000)
      process = FALSE
  }
  thetas_2e[i] <- theta
  counts_2e[i] <- count
  lvals_2e[i] <- llikeli(theta, x. = x)
}

thetas_2e <- matrix(thetas_2e, ncol = length(startp))
counts_2e <- matrix(counts_2e, ncol = length(startp))
lvals_2e <- matrix(lvals_2e, ncol = length(startp))
colnames(thetas_2e) <- round(startp, digits = 2)
colnames(counts_2e) <- round(startp, digits = 2)
colnames(lvals_2e) <- round(startp, digits = 2)
```

The graph of the MLE results is shown below.
```{r q2egraph}
plot(startp, thetas_2e, main='MLE Results', cex = 0.5,
     ylab='MLE', xlab='Starting Points', pch = 1)
```

To divide the set of starting values into separate groups, with each group corresponding to a separate unique outcome of the optimization. We use the "unique" function to receive the index (the start value) of each group.
```{r grouping}
thetas_2ee <- as.data.frame(round(t(thetas_2e), digits = 2))
colnames(thetas_2ee) <- c("MLE")
uniq <- unique(thetas_2ee)
knitr::kable(uniq)
```



####Q3

##a
```{r q3a}
rm(list=ls())
beetles <- data.frame(
  days = c(0, 8, 28, 41, 63, 69, 97, 117, 135, 154),
  beetles = c(2, 47, 192, 256, 768, 896, 1120, 896, 1184, 1024))

errors <- rep(NA, nrow(beetles))

nls(beetles ~ (K*beetles[1])/(beetles[1]+(K-beetles[1])*exp(-r*days)), 
    start = list(K = 1000, r = 1), data = beetles, trace = TRUE)
```

We received K = 1049.4068, r = 0.1183, residual sum-of-sqaures = 73420.

##b
```{r q3b,echo=FALSE}
###q3b
gnapp <- function(r, K){
  for (i in 1:nrow(beetles)){
    errors[i] <- (beetles$beetles[i] - (K * beetles$beetles[1]) /
                    (beetles$beetles[1] + (K - beetles$beetles[1])*exp(-r * beetles$days[i])))^2
  }
  return(sum(errors))
}

z <- matrix(0,100,100,byrow = TRUE)
for (i in 1:100){
  for(j in 1:100){
    K <- 0+12*j
    r <- 0+0.01*i
    z[j,i] <- gnapp(r, K)
  }
}

r <- seq(0,1,length.out = 100)
K <- seq(0,1200,length.out = 100)
contour(K,r,z, xlab = "K", ylab = "r")
```

##c

We first write the codes for the partial derivatives about K, r, sigma.

```{r q3c,echo=FALSE}
###q3c
rm(list=ls())
l <- expression(
  log(1/(sqrt(2*pi)*sigma))-(log((2*2+2*(K-2)*exp(-r*0))/(2*K)))^2/(2*sigma^2)+
  log(1/(sqrt(2*pi)*sigma))-(log((2*47+47*(K-2)*exp(-r*8))/(2*K)))^2/(2*sigma^2)+
  log(1/(sqrt(2*pi)*sigma))-(log((2*192+192*(K-2)*exp(-r*28))/(2*K)))^2/(2*sigma^2)+
  log(1/(sqrt(2*pi)*sigma))-(log((2*256+256*(K-2)*exp(-r*41))/(2*K)))^2/(2*sigma^2)+
  log(1/(sqrt(2*pi)*sigma))-(log((2*768+768*(K-2)*exp(-r*63))/(2*K)))^2/(2*sigma^2)+
  log(1/(sqrt(2*pi)*sigma))-(log((2*896+896*(K-2)*exp(-r*69))/(2*K)))^2/(2*sigma^2)+
  log(1/(sqrt(2*pi)*sigma))-(log((2*1120+1120*(K-2)*exp(-r*97))/(2*K)))^2/(2*sigma^2)+
  log(1/(sqrt(2*pi)*sigma))-(log((2*896+896*(K-2)*exp(-r*117))/(2*K)))^2/(2*sigma^2)+
  log(1/(sqrt(2*pi)*sigma))-(log((2*1184+1184*(K-2)*exp(-r*135))/(2*K)))^2/(2*sigma^2)+
  log(1/(sqrt(2*pi)*sigma))-(log((2*1024+1024*(K-2)*exp(-r*154))/(2*K)))^2/(2*sigma^2))

lpk <- D(l,"K")
lpr <- D(l,"r")
lps <- D(l,"sigma")
lppkk <- D(D(l,"K"),"K")
lppkr <- D(D(l,"K"),"r")
lppks <- D(D(l,"K"),"sigma")
lpprr <- D(D(l,"r"),"r")
lpprs <- D(D(l,"r"),"sigma")
lppss <- D(D(l,"sigma"),"sigma")
```

Then determine the initial value of K = 1050, r = 0.12, and sigma = 0,5.
```{r krs}
krs <- matrix(c(1050, 0.12, 0.5))
row.names(krs) <- c("K", "r", "sigma")
knitr::kable(krs)
```

By using the Gauss-Newton method, after 8 times, we received the maximum likelihood estimators of {K, r, $\sigma$} and {K, r, $\sigma^2$}
```{r krss, echo=FALSE}
count <- 0
process <- TRUE
while(process){
  K <- krs[1]
  r <- krs[2]
  sigma <- krs[3]
  gp <- matrix(c(eval(lpk), eval(lpr), eval(lps)))
  gpt <- t(gp)
  M <- matrix(c(eval(lppkk),eval(lppkr),eval(lppks),eval(lppkr),eval(lpprr),
                eval(lpprs),eval(lppks),eval(lpprs),eval(lppss)),byrow=TRUE,nrow=3)
  Minv <- solve(M)
  krs <-  krs - Minv %*% gp
  count <- count + 1
  if(gpt%*%gp < 1e-6 | count == 1000)
    process = FALSE
}
count

knitr::kable(t(krs))
krss <- matrix(c(K,r,sigma^2), ncol = 3)
colnames(krss) <- c("K", "r", "sigma2")
knitr::kable(krss)
```

From the previous knowledge, we know that the information matrix of parameter estimates is the negative inverse matrix of fisher information, and the values on the diagnol of the information matrix are the variance of parameter estimates.
```{r var}
vari <- solve(-M)
colnames(vari) <- row.names(vari) <- c("K", "r", "sigma")
knitr::kable(vari)
```


